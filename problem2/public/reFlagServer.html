<!DOCTYPE html>
<html lang="ko">
  <head>
    <meta charset="UTF-8" />
    <title>Muxed A/V Streaming</title>
    <style>
      body {
        margin: 0;
        display: flex;
        flex-direction: column;
        align-items: center;
      }
      #controls {
        margin: 10px;
      }
      #videoCanvas {
        background: #000;
      }
    </style>
  </head>
  <body>
    <div id="controls">
      <input
        id="urlInput"
        size="40"
        value="http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ElephantsDream.mp4"
      />
      <button id="startBtn">시작</button>
      &nbsp;|&nbsp;
      <input id="seekInput" type="number" value="0" style="width: 60px" />초
      <button id="seekBtn">Seek</button>
    </div>
    <canvas id="videoCanvas"></canvas>

    <script>
      const urlInput = document.getElementById("urlInput");
      const startBtn = document.getElementById("startBtn");
      const seekInput = document.getElementById("seekInput");
      const seekBtn = document.getElementById("seekBtn");
      const canvas = document.getElementById("videoCanvas");
      const ctx = canvas.getContext("2d");

      let socket;
      let firstServerTs = 0; // ✨ 첫 패킷의 서버 타임스탬프 저장용
      let startLocal, startAudioTime;
      // VideoDecoder 상태
      let decoder,
        annexbBuffer = new Uint8Array(),
        configured = false,
        sps,
        pps;
      // AudioContext 상태
      const audioCtx = new AudioContext();
      let audioTime = 0;

      // 1) AVCDecoderConfigurationRecord 생성
      function createConfig(sps, pps) {
        const spsLen = sps.byteLength,
          ppsLen = pps.byteLength;
        const desc = new Uint8Array(7 + 2 + spsLen + 1 + 2 + ppsLen);
        let o = 0;
        desc[o++] = 1;
        desc[o++] = sps[1];
        desc[o++] = sps[2];
        desc[o++] = sps[3];
        desc[o++] = 0xff;
        desc[o++] = 0xe1;
        desc[o++] = (spsLen >> 8) & 0xff;
        desc[o++] = spsLen & 0xff;
        desc.set(sps, o);
        o += spsLen;
        desc[o++] = 1;
        desc[o++] = (ppsLen >> 8) & 0xff;
        desc[o++] = ppsLen & 0xff;
        desc.set(pps, o);
        return desc;
      }

      function handleFrame(frame) {
        // 1) 현재 로컬 epoch 시간
        const nowEpoch = performance.timeOrigin + performance.now();
        // 2) E2E 지연 = draw시점 – 서버 송출시점
        const e2e = (nowEpoch - frame.timestamp).toFixed(1);

        // 3) 그리기
        canvas.width = frame.codedWidth;
        canvas.height = frame.codedHeight;
        ctx.imageSmoothingEnabled = false;
        ctx.drawImage(frame, 0, 0);

        // 4) 오버레이
        ctx.font = "20px sans-serif";
        ctx.fillStyle = "yellow";
        ctx.fillText(`E2E: ${e2e} ms`, 10, 30);

        frame.close();
      }
      // 2) VideoDecoder 초기화
      async function initDecoder(desc) {
        decoder = new VideoDecoder({
          output: handleFrame,
          error: (e) => console.error("Decoder error:", e),
        });
        decoder.configure({ codec: "avc1.42E01E", description: desc });
        console.log("▶ decoder configured");
      }

      // 3) 디코더 리셋
      function resetDecoder() {
        if (decoder && decoder.state !== "closed") {
          decoder.close();
          console.log("▶ decoder closed");
        }
        annexbBuffer = new Uint8Array();
        configured = false;
        sps = pps = undefined;
      }

      // 4) WebSocket 연결 및 A/V 처리
      function connect() {
        // 초기화
        // 사용자 액션 직후 AudioContext resume

        if (audioCtx.state === "suspended") {
          audioCtx.resume().then(() => console.log("▶ audioCtx resumed"));
        }
        startLocal = performance.now();
        startAudioTime = audioCtx.currentTime;
        resetDecoder();
        audioTime = startAudioTime;
        socket?.close();

        const wsUrl = `ws://${location.host}/stream?src=${encodeURIComponent(
          urlInput.value
        )}`;
        socket = new WebSocket(wsUrl);
        socket.binaryType = "arraybuffer";

        socket.addEventListener("open", () => console.log("▶ WS open"));
        socket.addEventListener("close", () => console.log("▶ WS closed"));
        socket.addEventListener("error", (e) => console.error("▶ WS error", e));

        socket.addEventListener("message", async (ev) => {
          // 1) parse header
          const packet = new Uint8Array(ev.data);
          const type = packet[0]; // 0x01=video,0x02=audio
          const tsDv = new DataView(packet.buffer, packet.byteOffset + 1, 8);
          // 서버 타임스탬프(ms)
          const serverTs = Number(tsDv.getBigUint64(0, false));
          // 실제 payload
          const body = packet.subarray(1 + 8);
          // ✨ 첫 타임스탬프 기록
          if (!firstServerTs) {
            firstServerTs = serverTs;
            startLocal = performance.now();
            startAudioTime = audioCtx.currentTime;
          }
          if (type === 1) {
            // ▶ Video: Annex‑B 누적 → NAL 분리 → configure/decode
            const tmp = new Uint8Array(
              annexbBuffer.byteLength + body.byteLength
            );
            tmp.set(annexbBuffer, 0);
            tmp.set(body, annexbBuffer.byteLength);
            annexbBuffer = tmp;

            const idxs = [];
            for (let i = 0; i + 3 < annexbBuffer.length; i++) {
              if (
                annexbBuffer[i] === 0 &&
                annexbBuffer[i + 1] === 0 &&
                annexbBuffer[i + 2] === 0 &&
                annexbBuffer[i + 3] === 1
              )
                idxs.push(i);
            }
            for (let i = 0; i + 1 < idxs.length; i++) {
              const nal = annexbBuffer.subarray(idxs[i] + 4, idxs[i + 1]);
              const typeNAL = nal[0] & 0x1f;

              if (!configured) {
                if (typeNAL === 7) {
                  sps = nal;
                  console.log("▶ got SPS");
                } else if (typeNAL === 8) {
                  pps = nal;
                  console.log("▶ got PPS");
                }
                if (sps && pps) {
                  await initDecoder(createConfig(sps, pps));
                  configured = true;
                }
                continue;
              }
              if (typeNAL !== 1 && typeNAL !== 5) continue; // slice/idr 만

              const len = nal.byteLength;
              const framed = new Uint8Array(4 + len);
              framed[0] = (len >>> 24) & 0xff;
              framed[1] = (len >>> 16) & 0xff;
              framed[2] = (len >>> 8) & 0xff;
              framed[3] = len & 0xff;
              framed.set(nal, 4);
              const recvTime = performance.now();
              const relTime = recvTime - startLocal;
              decoder.decode(
                new EncodedVideoChunk({
                  type: typeNAL === 5 ? "key" : "delta",
                  timestamp: recvTime,
                  data: framed,
                })
              );
            }
            if (idxs.length)
              annexbBuffer = annexbBuffer.subarray(idxs[idxs.length - 1]);
          } else if (type === 2) {
            // ▶ Audio: 타임스탬프 기반으로 재생 스케줄링 (✨ 이 부분이 완전히 변경됨)
            const pcmBytes = body.slice();
            const pcm16 = new Int16Array(pcmBytes.buffer);

            const frameCount = pcm16.length / 2;
            const left = new Float32Array(frameCount);
            const right = new Float32Array(frameCount);
            for (let i = 0, j = 0; j < frameCount; i += 2, j++) {
              left[j] = pcm16[i] / 32768;
              right[j] = pcm16[i + 1] / 32768;
            }

            const buf = audioCtx.createBuffer(2, frameCount, 48000);
            buf.copyToChannel(left, 0);
            buf.copyToChannel(right, 1);

            // 스트림 시작 후 지난 시간(초) 계산
            const mediaTimeSec = (serverTs - firstServerTs) / 1000;
            // 오디오 컨텍스트 시작 시간에 오차를 더해 재생 시간 예약
            const playTime = startAudioTime + mediaTimeSec;

            const src = audioCtx.createBufferSource();
            src.buffer = buf;
            src.connect(audioCtx.destination);

            // 너무 늦게 도착한 오디오는 버리고 현재 재생 시간보다 살짝 앞에서 재생
            if (playTime < audioCtx.currentTime) {
              src.start(audioCtx.currentTime + 0.01);
            } else {
              src.start(playTime);
            }
          }
        });
      }

      startBtn.onclick = connect;

      // 5) Seek
      seekBtn.onclick = () => {
        const t = parseFloat(seekInput.value);
        if (!isNaN(t) && socket?.readyState === WebSocket.OPEN) {
          console.log("▶ send seek", t);
          socket.send(JSON.stringify({ type: "seek", time: t }));
          resetDecoder();
          audioTime = audioCtx.currentTime;
        }
      };
    </script>
  </body>
</html>
